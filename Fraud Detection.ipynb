{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390cf521",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "The dataset can be accessed from https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?resource=download. It consists of genuine bank transactions conducted by cardholders in Europe in 2013. For security reasons, the specific variables have not been disclosed and instead have been altered through PCA (Principal Component Analysis). Consequently, there are a total of 29 columns representing features and 1 column representing the final class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b6c6d",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "All necessary libraries are imported in one place. The credit card dataset features are already transformed through PCA, eliminating the need for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a05a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x216 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Packages related to general operating system & warnings\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Packages related to data importing, manipulation, exploratory data #analysis, data understanding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import csv\n",
    "#from termcolor import colored as cl # text customization\n",
    "#Packages related to data visualizaiton\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Setting plot sizes and type of plot\n",
    "plt.rc(\"font\", size=14)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.gray()\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "from sklearn.preprocessing import  PolynomialFeatures, KBinsDiscretizer, FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, OrdinalEncoder\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa as tsa\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor,RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor \n",
    "from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb54a5e8",
   "metadata": {},
   "source": [
    "We first read the CSV file \"creditcard.csv\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d869247d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"D:\\creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecea941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Transactions are 284807\n",
      "Number of Normal Transactions are 284315\n",
      "Number of fraudulent Transactions are 492\n",
      "Percentage of fraud Transactions is 0.17\n"
     ]
    }
   ],
   "source": [
    "Total_transactions = len(data)\n",
    "normal = len(data[data.Class == 0])\n",
    "fraudulent = len(data[data.Class == 1])\n",
    "fraud_percentage = round(fraudulent/normal*100, 2)\n",
    "print('Total number of Transactions are {}'.format(Total_transactions))\n",
    "print('Number of Normal Transactions are {}'.format(normal))\n",
    "print('Number of fraudulent Transactions are {}'.format(fraudulent))\n",
    "print('Percentage of fraud Transactions is {}'.format(fraud_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b233fb",
   "metadata": {},
   "source": [
    "Only 0.17% of transactions are fraudulent.\n",
    "\n",
    "Check for null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861dd925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb80ce",
   "metadata": {},
   "source": [
    "No null values exist in the columns, and feature selection is not applicable for this particular use case. However, it may be worth trying out feature selection mechanisms to potentially improve results. It's worth noting that out of the 28 features in our data, 27 are transformed through PCA, while the Amount feature remains original. After checking the minimum and maximum values in the Amount feature, it was discovered that the difference is significant enough to potentially skew our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11dcf5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 25691.16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min (data.Amount),max(data.Amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3042be",
   "metadata": {},
   "source": [
    "Scaling this variable using a standard scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a470e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "amount = data['Amount'].values\n",
    "data['Amount'] = sc.fit_transform(amount.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7c89b",
   "metadata": {},
   "source": [
    "The fit_transform() method of StandardScaler first fits the scaler to the data by computing the mean and standard deviation of the input values, and then scales the values using these parameters to obtain the standardized values. The reshaping of the amount array is necessary because fit_transform() expects a 2D array as input. Finally, the scaled values are assigned back to the Amount column in the data dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b52beb0",
   "metadata": {},
   "source": [
    "We have one more variable which is the time which can be an external deciding factor â€” but in our modelling process, we can drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fddcc179",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcd0b47",
   "metadata": {},
   "source": [
    "We can identify duplicate transactions in our dataset. After removal, the original 284807 transactions will be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f67220b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02815ef",
   "metadata": {},
   "source": [
    "Now, removing any duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edea2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbecab79",
   "metadata": {},
   "source": [
    "Performing a count verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a96ee60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275663, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43052bd1",
   "metadata": {},
   "source": [
    "We initially had approximately 9000 instances of duplicated transactions. However, we have now successfully preprocessed our data by scaling it and removing any instances of duplicates or missing values. We can now proceed to partition our data for use in constructing our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d1d28",
   "metadata": {},
   "source": [
    "## Train & Test Split\n",
    "In order to split data into training and testing sets, we first established the independent and dependent variables. The independent variable, commonly referred to as \"y\", is the input to a machine learning model, whereas the dependent variable, also known as \"X\", is the output that is predicted by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa339e2",
   "metadata": {},
   "source": [
    "We first create the input feature matrix X and target vector y for the machine learning models as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c895a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Class', axis = 1).values\n",
    "y = data['Class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3673a40",
   "metadata": {},
   "source": [
    "The drop() method of a dataframe is used to remove one or more columns from the dataframe. In this case, the 'Class' column is dropped along the vertical axis (axis=1), and the resulting dataframe is converted to a numpy array using the values attribute. This array is assigned to the variable X. The 'Class' column values are extracted as a numpy array using the values attribute and assigned to the variable y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f2bffe",
   "metadata": {},
   "source": [
    "Now, we split our train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a1fb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c09834",
   "metadata": {},
   "source": [
    "The train_test_split() function randomly splits the data into training and test sets. The X and y arrays are provided as the first two arguments to the function. The test_size parameter is set to 0.25, which means that 25% of the data will be used for testing and 75% for training. The random_state parameter is set to 1, which ensures that the same random split is generated every time the code is run. The resulting training and test sets are assigned to the variables X_train, X_test, y_train, and y_test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333a0d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5697152",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "We will conduct a sequential evaluation of various machine learning algorithms and optimize their parameters to enhance their performance. This are then compared to find out the model with the highest fraud detection prediction accuracy and the F1-score.\n",
    "\n",
    "F1-score is a metric that combines precision and recall of a model to provide a single score that summarizes the model's performance. It is calculated as the harmonic mean of precision and recall. In the context of evaluating classification models for credit card fraud detection, F1-score can be used to measure the overall performance of the model in detecting both fraudulent and non-fraudulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af027509",
   "metadata": {},
   "source": [
    "## 1. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1136400d",
   "metadata": {},
   "source": [
    "We fit the Decision Tree model with a maximum depth of 4 and using entropy as the splitting criterion on the training set. We then predict the labels of the test set using the predict() method and store the result in dt_yhat. Heres the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c88bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\n",
    "DT.fit(X_train, y_train)\n",
    "dt_yhat = DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f7ab8a",
   "metadata": {},
   "source": [
    "Checking the accuracy of our decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eea5cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Decision Tree model is 0.9991438853096524\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test, dt_yhat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6ca59",
   "metadata": {},
   "source": [
    "Checking F1-Score for the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e269582d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the Decision Tree model is 0.7467811158798283\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the Decision Tree model is {}'.format(f1_score(y_test, dt_yhat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15715e1c",
   "metadata": {},
   "source": [
    "Checking the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5537dbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[68770,    18],\n",
       "       [   41,    87]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, dt_yhat, labels = [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be3892",
   "metadata": {},
   "source": [
    "The first row denotes true positives, while the second row denotes true negatives. The results show 68782 true positives and 18 false positives. False positives are the number of normal transactions predicted as fraud, true negatives are the number of correctly predicted normal transactions, and false negatives are the number of fraud transactions predicted as normal.\n",
    "\n",
    "This indicates that out of a total of 68800 cases, 68782 were correctly identified as normal transactions, but 18 were falsely classified as normal, despite being fraudulent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1690d2e",
   "metadata": {},
   "source": [
    "## 2. K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3d2ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "# fit a K-Nearest Neighbors model to the training data\n",
    "knn = KNeighborsClassifier(n_neighbors = n)\n",
    "knn.fit(X_train, y_train)\n",
    "# make predictions on the test set\n",
    "knn_yhat = knn.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, knn_yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ffa161",
   "metadata": {},
   "source": [
    "The dataset is split into training and test sets using train_test_split(). A K-Nearest Neighbors model is then fit to the training data using KNeighborsClassifier(). Predictions are made on the test set using predict(), and the accuracy score is calculated using accuracy_score(). Finally, the accuracy score is printed using print()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874354da",
   "metadata": {},
   "source": [
    "Checking accuracy of K-Nearest Neighbors model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "850a66c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the K-Nearest Neighbors model is 0.999288989494457\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the K-Nearest Neighbors model is {}'.format(acc_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21a00a",
   "metadata": {},
   "source": [
    "Checking F1-Score for the K-Nearest Neighbors model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49b49ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the K-Nearest Neighbors model is 0.7949790794979079\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, knn_yhat)\n",
    "print('F1 score of the K-Nearest Neighbors model is {}'.format(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c26aa9",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2f0a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_yhat = lr.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, lr_yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af77d0",
   "metadata": {},
   "source": [
    "A Logistic Regression model is fit to the training data using LogisticRegression(). Predictions are made on the test set using predict(), and the F1 score is calculated using f1_score(). Finally, the F1 score is printed using print()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca97ca",
   "metadata": {},
   "source": [
    "Checking the accuracy of the Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44648dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Logistic Regression model is 0.9989552498694062\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the Logistic Regression model is {}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec65c4d4",
   "metadata": {},
   "source": [
    "Checking F1-Score for the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "232c826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the Logistic Regression model is 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the Logistic Regression model is {}'.format(f1_score(y_test, lr_yhat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee5208f",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "005c9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svm_yhat = svm.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, svm_yhat)\n",
    "f1 = f1_score(y_test, svm_yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ab902",
   "metadata": {},
   "source": [
    "An SVM model is then fit to the training data using SVC(). Predictions are made on the test set using predict(), and the accuracy and F1 scores are calculated using accuracy_score() and f1_score(), respectively. Finally, the accuracy and F1 scores are printed using print()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc3473",
   "metadata": {},
   "source": [
    "Checking the accuracy of our SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16cf0198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Support Vector Machines model is 0.999318010331418\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the Support Vector Machines model is {}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c4c50c",
   "metadata": {},
   "source": [
    "Checking F1-Score for the SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b41affd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the Support Vector Machines model is 0.7813953488372093\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the Support Vector Machines model is {}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f2b48f",
   "metadata": {},
   "source": [
    "## 5. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d04185fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth = 4)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_yhat = rf.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, rf_yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c2827",
   "metadata": {},
   "source": [
    "The Random Forest Classifier model is initialized using a maximum depth of 4. A Random Forest Classifier model is then fit to the training data using RandomForestClassifier(). Predictions are made on the test set using predict(), and the accuracy score is calculated using accuracy_score(). Finally, the accuracy score is printed using print()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c43ab",
   "metadata": {},
   "source": [
    "Checking the accuracy of the Random Forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fa323cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Random Forest model is 0.9991293748911718\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the Random Forest model is {}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95898eea",
   "metadata": {},
   "source": [
    "## 6. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e5030c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth = 4)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_yhat = xgb.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, xgb_yhat)\n",
    "f1 = f1_score(y_test, xgb_yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1b51e",
   "metadata": {},
   "source": [
    "An XGBoost Classifier model is then fit to the training data using XGBClassifier(). Predictions are made on the test set using predict(), and the accuracy score and F1 score are calculated using accuracy_score() and f1_score(), respectively. Finally, the accuracy score and F1 score are printed using print()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed668c5f",
   "metadata": {},
   "source": [
    "Checking accuracy of our XGBoost model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "125d66c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the XGBoost model is 0.999506645771664\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the XGBoost model is {}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd18f5c6",
   "metadata": {},
   "source": [
    "Checking F1-Score for the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b986615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the XGBoost model is 0.8495575221238937\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the XGBoost model is {}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292a49d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The credit card fraud detection model achieves an accuracy of 99.95%, primarily due to class imbalance in the data. The confusion matrix indicates the model is not overfitting. XGBoost has the highest accuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13987e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
